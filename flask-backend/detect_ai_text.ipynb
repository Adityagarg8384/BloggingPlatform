{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6300093,"sourceType":"datasetVersion","datasetId":3623988},{"sourceId":6571530,"sourceType":"datasetVersion","datasetId":3796024},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":6853624,"sourceType":"datasetVersion","datasetId":3939470},{"sourceId":6857742,"sourceType":"datasetVersion","datasetId":3623154},{"sourceId":6865136,"sourceType":"datasetVersion","datasetId":3945154},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":6971638,"sourceType":"datasetVersion","datasetId":3961875},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":6987454,"sourceType":"datasetVersion","datasetId":3947266},{"sourceId":7018354,"sourceType":"datasetVersion","datasetId":4035516},{"sourceId":7069044,"sourceType":"datasetVersion","datasetId":4070722},{"sourceId":7139426,"sourceType":"datasetVersion","datasetId":4120366},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":150784240,"sourceType":"kernelVersion"},{"sourceId":159282003,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport gc\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:10.349774Z","iopub.execute_input":"2024-05-19T10:53:10.350079Z","iopub.status.idle":"2024-05-19T10:53:17.493305Z","shell.execute_reply.started":"2024-05-19T10:53:10.350052Z","shell.execute_reply":"2024-05-19T10:53:17.492360Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\norg_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:17.494978Z","iopub.execute_input":"2024-05-19T10:53:17.495624Z","iopub.status.idle":"2024-05-19T10:53:19.756033Z","shell.execute_reply.started":"2024-05-19T10:53:17.495597Z","shell.execute_reply":"2024-05-19T10:53:19.755017Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.rename(columns={'label':'generated'},inplace=True)\ntrain.drop(columns=['prompt_name','source','RDizzl3_seven'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:19.757263Z","iopub.execute_input":"2024-05-19T10:53:19.757556Z","iopub.status.idle":"2024-05-19T10:53:19.776277Z","shell.execute_reply.started":"2024-05-19T10:53:19.757531Z","shell.execute_reply":"2024-05-19T10:53:19.775048Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_new_a=pd.read_csv(\"/kaggle/input/10-74k-detect-ai-generated-text/essays_a.csv\")\ntrain_new_b=pd.read_csv(\"/kaggle/input/10-74k-detect-ai-generated-text/essays_b.csv\")\ntrain_new_c=pd.read_csv(\"/kaggle/input/10-74k-detect-ai-generated-text/essays_c.csv\")\ntrain_new_d=pd.read_csv(\"/kaggle/input/10-74k-detect-ai-generated-text/essays_d.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:19.779020Z","iopub.execute_input":"2024-05-19T10:53:19.779381Z","iopub.status.idle":"2024-05-19T10:53:20.367607Z","shell.execute_reply.started":"2024-05-19T10:53:19.779353Z","shell.execute_reply":"2024-05-19T10:53:20.366583Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_new_a.drop(columns=['prompt_name','prompt_id'],inplace=True)\ntrain_new_b.drop(columns=['prompt_name','prompt_id'],inplace=True)\ntrain_new_c.drop(columns=['prompt_name','prompt_id'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:20.368622Z","iopub.execute_input":"2024-05-19T10:53:20.368905Z","iopub.status.idle":"2024-05-19T10:53:20.376174Z","shell.execute_reply.started":"2024-05-19T10:53:20.368881Z","shell.execute_reply":"2024-05-19T10:53:20.375195Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train=pd.concat([train_new_a,train_new_b,train_new_c,train_new_d,train])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:20.377215Z","iopub.execute_input":"2024-05-19T10:53:20.377446Z","iopub.status.idle":"2024-05-19T10:53:20.392770Z","shell.execute_reply.started":"2024-05-19T10:53:20.377425Z","shell.execute_reply":"2024-05-19T10:53:20.391987Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\n\ntrain.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:20.393761Z","iopub.execute_input":"2024-05-19T10:53:20.394098Z","iopub.status.idle":"2024-05-19T10:53:20.479261Z","shell.execute_reply.started":"2024-05-19T10:53:20.394068Z","shell.execute_reply":"2024-05-19T10:53:20.478392Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 30522","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:20.480317Z","iopub.execute_input":"2024-05-19T10:53:20.480620Z","iopub.status.idle":"2024-05-19T10:53:20.488485Z","shell.execute_reply.started":"2024-05-19T10:53:20.480593Z","shell.execute_reply":"2024-05-19T10:53:20.487601Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n# Adding normalization and pre_tokenizer\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n# Adding special tokens and creating trainer instance\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n# Creating huggingface dataset object\ndataset = Dataset.from_pandas(test[['text']])\ndef train_corp_iter(): \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n\nimport pickle\nwith open('raw_vectorizer.pk', 'wb') as fin:\n    pickle.dump(raw_tokenizer, fin)\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\ntokenized_texts_test = []\n\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T10:53:47.382969Z","iopub.execute_input":"2024-05-19T10:53:47.383371Z","iopub.status.idle":"2024-05-19T10:56:32.428139Z","shell.execute_reply.started":"2024-05-19T10:53:47.383342Z","shell.execute_reply":"2024-05-19T10:56:32.427047Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ffa8aa3e1974a5c837a664672df3d17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/55608 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e159d33d2494fa2bced2b70f7339802"}},"metadata":{}}]},{"cell_type":"code","source":"def dummy(text):\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:01:53.101438Z","iopub.execute_input":"2023-12-06T18:01:53.102258Z","iopub.status.idle":"2023-12-06T18:01:53.1078Z","shell.execute_reply.started":"2023-12-06T18:01:53.102218Z","shell.execute_reply":"2023-12-06T18:01:53.106677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n    tokenizer = dummy,\n    preprocessor = dummy,\n    token_pattern = None, strip_accents='unicode'\n                            )\n\nvectorizer.fit(tokenized_texts_test)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk import word_tokenize\nimport pickle\nwith open('vectorizer.pk', 'wb') as fin:\n    pickle.dump(vectorizer, fin)\n# Getting vocab\nvocab = vectorizer.vocabulary_\n\nprint(vocab)\n\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train)\nwith open('vectorizer_1.pk', 'wb') as fina:\n    pickle.dump(vectorizer, fina)\ntf_test = vectorizer.transform(tokenized_texts_test)\n\ndel vectorizer\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:01:53.109039Z","iopub.execute_input":"2023-12-06T18:01:53.109412Z","iopub.status.idle":"2023-12-06T18:06:14.000479Z","shell.execute_reply.started":"2023-12-06T18:01:53.109385Z","shell.execute_reply":"2023-12-06T18:06:13.999072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['generated'].values","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:07:32.683505Z","iopub.execute_input":"2023-12-06T18:07:32.683964Z","iopub.status.idle":"2023-12-06T18:07:32.689218Z","shell.execute_reply.started":"2023-12-06T18:07:32.683932Z","shell.execute_reply":"2023-12-06T18:07:32.688186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = MultinomialNB(alpha=0.02)\nclf2 = MultinomialNB(alpha=0.01)\nsgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\") \np6={'n_iter': 1000,'verbose': -1,'learning_rate': 0.005689066836106983, 'colsample_bytree': 0.8915976762048253, 'colsample_bynode': 0.5942203285139224, 'lambda_l1': 7.6277555139102864, 'lambda_l2': 6.6591278779517808, 'min_data_in_leaf' : 156, 'max_depth': 11, 'max_bin': 813}\nlgb=LGBMClassifier(**p6)\n\nensemble = VotingClassifier(estimators=[('mnb',clf),\n                                        ('sgd', sgd_model),\n                                        ('lgb',lgb)\n                                       ],\n                            weights=[0.2,0.4,0.4], voting='soft', n_jobs=-1)\nensemble.fit(tf_train, y_train)\n\ngc.collect()\nimport pickle\nfilename = 'finalized_model.sav'\npickle.dump(ensemble, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-12-06T18:07:34.752579Z","iopub.execute_input":"2023-12-06T18:07:34.753515Z","iopub.status.idle":"2023-12-06T18:07:41.632912Z","shell.execute_reply.started":"2023-12-06T18:07:34.753481Z","shell.execute_reply":"2023-12-06T18:07:41.631541Z"},"trusted":true},"execution_count":null,"outputs":[]}]}